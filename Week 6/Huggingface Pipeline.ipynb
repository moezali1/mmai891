{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\owner\\anaconda3\\envs\\mma865\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9829559326171875}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"The food was good overall but Pizza was horrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998760223388672}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I am loving it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9970771074295044}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I hate waking up early on the weekends.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>I will be making a major statement from the @W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>Just arrived at #ASEAN50 in the Philippines fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>After my tour of Asia, all Countries dealing w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>Great to see @RandPaul looking well and back o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>Excited to be heading home to see the House pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           author                                             status\n",
       "0   1  Donald J. Trump  I will be making a major statement from the @W...\n",
       "1   2  Donald J. Trump  Just arrived at #ASEAN50 in the Philippines fo...\n",
       "2   3  Donald J. Trump  After my tour of Asia, all Countries dealing w...\n",
       "3   4  Donald J. Trump  Great to see @RandPaul looking well and back o...\n",
       "4   5  Donald J. Trump  Excited to be heading home to see the House pa..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('tweets_trump_trudeau.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9971588850021362}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.8045341968536377}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9773215651512146}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9997676014900208}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9989203214645386}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9989012479782104}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9992148876190186}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9909795522689819}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9979164004325867}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9990936517715454}]\n"
     ]
    }
   ],
   "source": [
    "for i in data['status'][:10]:\n",
    "    print(classifier(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owner\\anaconda3\\envs\\mma865\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier2 = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'anger', 'score': 0.5119701623916626}]\n",
      "[{'label': 'optimism', 'score': 0.48820018768310547}]\n",
      "[{'label': 'joy', 'score': 0.4762808680534363}]\n",
      "[{'label': 'anger', 'score': 0.5054358839988708}]\n",
      "[{'label': 'optimism', 'score': 0.8508567810058594}]\n",
      "[{'label': 'anger', 'score': 0.6616511940956116}]\n",
      "[{'label': 'optimism', 'score': 0.5628583431243896}]\n",
      "[{'label': 'anger', 'score': 0.744921863079071}]\n",
      "[{'label': 'optimism', 'score': 0.6141897439956665}]\n",
      "[{'label': 'optimism', 'score': 0.6337209343910217}]\n"
     ]
    }
   ],
   "source": [
    "for i in data['status'][:10]:\n",
    "    print(classifier2(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\owner\\anaconda3\\envs\\mma865\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The MA 865 2025B: Big Data Analytics course has two core components: Natural Language Processing (NLP) and Big Data Engineering (MLO) We will discuss the major practice areas of NLP and several of its use-cases across many different industries . We will explore big data engineering, covering key technologies such as Apache Hadoop, Spark, Relational and NonRelational Database, and Cloud platforms .'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\"\"\"\n",
    "¬¬MMA 865 2025B: Big Data Analytics\n",
    "Course Syllabus\n",
    "Updated August 7, 2024\n",
    "COURSE DESCRIPTION\n",
    "This course has two core components:\n",
    "1.\tNatural Language Processing\n",
    "Natural Language Processing (NLP) is one of the six AI disciplines. We will discuss the major practice areas of NLP and several of its use-cases across many different industries. These key areas include Information Extraction, Document Classification, Sentiment Analysis, Language Generation, Chatbots, and Machine Translation. \n",
    "We will thoroughly cover text preprocessing and vectorization, as they are foundations for training NLP models. You will gain comprehensive knowledge of the tools and techniques necessary for effectively managing text data. This includes text preprocessing, text data visualization, and text vectorization / embeddings.\n",
    "Large Language Models (LLM) like GPT have ushered in a paradigm shift in NLP, making them an indispensable asset for solving a wide range of language-related tasks. We will also briefly cover Transfer Learning and Transformer architecture in NLP, highlighting their significant impact on language models like GPT, Claude, LLaMa, Mistral, BARD, etc. Finally, we will explore how OpenAI's ChatGPT (and other LLM’s) are revolutionizing the operational landscape by studying practical use-cases of LLM’s.\n",
    "2.\tBig Data Engineering\n",
    "We will explore big data engineering, covering key technologies such as Apache Hadoop, Spark, Relational and Non-Relational Database, and Cloud platforms. You will learn the history of big data, fundamentals, and the current trends.\n",
    "We will also delve into advanced topics for machine learning system design. This includes an overview of several key technologies used in Machine Learning Operations (MLOps) such as Docker, Kubernetes, Microservices architecture, Experiment Logging, etc. Through a combination of theory and demos, you will gain practical insights into the foundations of big data engineering.\n",
    "INSTRUCTOR\n",
    "Moez Ali\n",
    "moez.ali@queensu.ca\n",
    "\tTEACHING ASSISTANT \n",
    "Raghav Gupta\n",
    "raghav.gupta@queensu.ca\n",
    "OFFICE HOURS\n",
    "Office hours schedule TBA in the first class.\n",
    "RECOMMENDED TEXTBOOKS\n",
    "Optional Textbooks that you can read at your own time.\n",
    "•\tTransformers for Natural Language Processing: Build, train, and fine-tune deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, and GPT-3, 2nd Edition\n",
    "•\tLearning Spark: Lightning-Fast Data Analytics 2nd Edition\n",
    "•\tModern Generative AI with ChatGPT and OpenAI Models 1st Edition - Leverage the capabilities of OpenAI's LLM for productivity and innovation with GPT3 and GPT4\n",
    "COURSE EVALUATION\n",
    "Item\tValue\tDue*\n",
    "Class Participation\t20%\tOn-going\n",
    "Individual Assignment\t30%\tSeptember 7, 2024\n",
    "Team Assignment\t30%\tSeptember 28, 2024\n",
    "Team Project\t20%\tOctober 5, 2024\n",
    "* By 11:59pm Eastern Time.\n",
    "1. Class Participation\n",
    "Class participation is integral to the learning process and constitutes 20% of the course grade. Effective participation goes beyond mere attendance; it requires active engagement in classroom discussions, demonstrating a thorough understanding of the course materials. Students are encouraged to actively listen, thoughtfully contribute to discussions, and engage with different viewpoints.\n",
    "A key aspect of class participation is the ability to ask meaningful questions that reflect deep engagement with the course content. Good questions are clear, concise, and stimulate further discussion. They should showcase critical thinking and a genuine curiosity about the subject matter.\n",
    "Sharing use-cases, real-life experience, a new information on the subject based on your expertise and as it relates to the content is another way to demonstrate strong class participation.\n",
    "2. Individual Assignment\n",
    "Please see Individual Assignment under “Assignment” section on the course portal. \n",
    "3. Team Assignment\n",
    "Please see Team Assignment – The Turing Tussle under “Assignment” section on the course portal.\n",
    "4. Team Project\n",
    "Please see Team Project - House of Common under “Assignment” section on the course portal.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Elon Musk was born on June 28, 1971, in Pretoria, South Africa . He was bullied until he was 15 and learned how to defend himself with karate and wrestling . He moved to Canada in 1989 to attend Queen’s University and avoid mandatory service in the South African military . He dropped out of Stanford University after two days to launch his first company, Zip2 .'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\"\"\"\n",
    "Elon Musk is a South African-born American entrepreneur and businessman who founded X.com in 1999 (which later became PayPal), SpaceX in 2002 and Tesla Motors in 2003. Musk became a multimillionaire in his late 20s when he sold his start-up company, Zip2, to a division of Compaq Computers. \n",
    "\n",
    "Musk made headlines in May 2012, when SpaceX launched a rocket that would send the first commercial vehicle to the International Space Station. He bolstered his portfolio with the purchase of SolarCity in 2016 and cemented his standing as a leader of industry by taking on an advisory role in the early days of President Donald Trump's administration.\n",
    "\n",
    "In January 2021, Musk reportedly surpassed Jeff Bezos as the wealthiest man in the world.\n",
    "\n",
    "Early Life\n",
    "Musk was born on June 28, 1971, in Pretoria, South Africa. As a child, Musk was so lost in his daydreams about inventions that his parents and doctors ordered a test to check his hearing.\n",
    "\n",
    "At about the time of his parents’ divorce, when he was 10, Musk developed an interest in computers. He taught himself how to program, and when he was 12 he sold his first software: a game he created called Blastar.\n",
    "\n",
    "In grade school, Musk was short, introverted and bookish. He was bullied until he was 15 and went through a growth spurt and learned how to defend himself with karate and wrestling.\n",
    "\n",
    "Family\n",
    "Musk’s mother, Maye Musk, is a Canadian model and the oldest woman to star in a Covergirl campaign. When Musk was growing up, she worked five jobs at one point to support her family.\n",
    "\n",
    "Musk’s father, Errol Musk, is a wealthy South African engineer.\n",
    "\n",
    "Musk spent his early childhood with his brother Kimbal and sister Tosca in South Africa. His parents divorced when he was 10.\n",
    "\n",
    "Education\n",
    "At age 17, in 1989, Musk moved to Canada to attend Queen’s University and avoid mandatory service in the South African military. Musk obtained his Canadian citizenship that year, in part because he felt it would be easier to obtain American citizenship via that path.\n",
    "\n",
    "In 1992, Musk left Canada to study business and physics at the University of Pennsylvania. He graduated with an undergraduate degree in economics and stayed for a second bachelor’s degree in physics.\n",
    "\n",
    "After leaving Penn, Musk headed to Stanford University in California to pursue a PhD in energy physics. However, his move was timed perfectly with the Internet boom, and he dropped out of Stanford after just two days to become a part of it, launching his first company, Zip2 Corporation in 1995. Musk became a U.S. citizen in 2002.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer2 = pipeline('summarization', model='facebook/bart-large-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Elon Musk is the wealthiest man in the world, according to Forbes magazine.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer2(\"\"\"\n",
    "Elon Musk is a South African-born American entrepreneur and businessman who founded X.com in 1999 (which later became PayPal), SpaceX in 2002 and Tesla Motors in 2003. Musk became a multimillionaire in his late 20s when he sold his start-up company, Zip2, to a division of Compaq Computers. \n",
    "\n",
    "Musk made headlines in May 2012, when SpaceX launched a rocket that would send the first commercial vehicle to the International Space Station. He bolstered his portfolio with the purchase of SolarCity in 2016 and cemented his standing as a leader of industry by taking on an advisory role in the early days of President Donald Trump's administration.\n",
    "\n",
    "In January 2021, Musk reportedly surpassed Jeff Bezos as the wealthiest man in the world.\n",
    "\n",
    "Early Life\n",
    "Musk was born on June 28, 1971, in Pretoria, South Africa. As a child, Musk was so lost in his daydreams about inventions that his parents and doctors ordered a test to check his hearing.\n",
    "\n",
    "At about the time of his parents’ divorce, when he was 10, Musk developed an interest in computers. He taught himself how to program, and when he was 12 he sold his first software: a game he created called Blastar.\n",
    "\n",
    "In grade school, Musk was short, introverted and bookish. He was bullied until he was 15 and went through a growth spurt and learned how to defend himself with karate and wrestling.\n",
    "\n",
    "Family\n",
    "Musk’s mother, Maye Musk, is a Canadian model and the oldest woman to star in a Covergirl campaign. When Musk was growing up, she worked five jobs at one point to support her family.\n",
    "\n",
    "Musk’s father, Errol Musk, is a wealthy South African engineer.\n",
    "\n",
    "Musk spent his early childhood with his brother Kimbal and sister Tosca in South Africa. His parents divorced when he was 10.\n",
    "\n",
    "Education\n",
    "At age 17, in 1989, Musk moved to Canada to attend Queen’s University and avoid mandatory service in the South African military. Musk obtained his Canadian citizenship that year, in part because he felt it would be easier to obtain American citizenship via that path.\n",
    "\n",
    "In 1992, Musk left Canada to study business and physics at the University of Pennsylvania. He graduated with an undergraduate degree in economics and stayed for a second bachelor’s degree in physics.\n",
    "\n",
    "After leaving Penn, Musk headed to Stanford University in California to pursue a PhD in energy physics. However, his move was timed perfectly with the Internet boom, and he dropped out of Stanford after just two days to become a part of it, launching his first company, Zip2 Corporation in 1995. Musk became a U.S. citizen in 2002.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The covenant was made with the people of Lower Canada, and recorded in the Statute Book of the United Kingdom of Great Britain and Ireland, as the thirty-first chapter of the Act passed in the 30-first year of the Reign of King George III . And, whereas our humble petitions, addresses, protests, and remonstrances against this injurious and unconstitutional interference have been made in vain. That the British Government has disposed of our revenue without the constitutional consent of the local Legislature .'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\"\"\"\n",
    "WHEREAS, the solemn covenant made with the people of Lower Canada, and recorded in the Statute Book of the United Kingdom of Great Britain and Ireland, as the thirty-first chapter of the Act passed in the thirty-first year of the Reign of King George III hath been continually violated by the British Government, and our rights usurped. And, whereas our humble petitions, addresses, protests, and remonstrances against this injurious and unconstitutional interference have been made in vain. That the British Government hath disposed of our revenue without the constitutional consent of the local Legislature — pillaged our treasury — arrested great numbers of our citizens, and committed them to prison — distributed through the country a mercenary army, whose presence is accompanied by consternation and alarm — whose track is red with the blood of our people — who have laid our villages in ashes — profaned our temples — and spread terror and waste through the land. And, whereas we can no longer suffer the repeated violations of our dearest rights, and patiently support the multiplied outrages and cruelties of the Government of Lower Canada, we, in the name of the people of Lower Canada, acknowledging the decrees of a Divine Providence, which permits us to put down a Government, which hath abused the object and intention for which it was created, and to make choice of that form of Government which shall re-establish the empire of justice — assure domestic tranquillity — provide for common defence — promote general good, and secure to us and our posterity the advantages of civil and religious liberty,\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'A petition signed by the people of Lower Canada, calling on the British Government to repeal the Indian Act.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer2(\"\"\"\n",
    "WHEREAS, the solemn covenant made with the people of Lower Canada, and recorded in the Statute Book of the United Kingdom of Great Britain and Ireland, as the thirty-first chapter of the Act passed in the thirty-first year of the Reign of King George III hath been continually violated by the British Government, and our rights usurped. And, whereas our humble petitions, addresses, protests, and remonstrances against this injurious and unconstitutional interference have been made in vain. That the British Government hath disposed of our revenue without the constitutional consent of the local Legislature — pillaged our treasury — arrested great numbers of our citizens, and committed them to prison — distributed through the country a mercenary army, whose presence is accompanied by consternation and alarm — whose track is red with the blood of our people — who have laid our villages in ashes — profaned our temples — and spread terror and waste through the land. And, whereas we can no longer suffer the repeated violations of our dearest rights, and patiently support the multiplied outrages and cruelties of the Government of Lower Canada, we, in the name of the people of Lower Canada, acknowledging the decrees of a Divine Providence, which permits us to put down a Government, which hath abused the object and intention for which it was created, and to make choice of that form of Government which shall re-establish the empire of justice — assure domestic tranquillity — provide for common defence — promote general good, and secure to us and our posterity the advantages of civil and religious liberty,\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0d1711e8d94798a2c076253c57ae99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owner\\anaconda3\\envs\\mma865\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\owner\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c322d93d9e4b9aa2849affd4e8da9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022e8ba800c942f7af7091bd2da9142e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd53da65e524f5187f49a0970d46f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6893ba5e48c543958ff5c62bb63420b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a539eb4c384b3ca25b50dd4c89a64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd37ae3bb294dfb83c0c9e72b329329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = pipeline('text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Adam and Natalie are good friends. Since then Natalie has tried the 'Jamaican' diet to become the best, but he has only had one sandwich so far. When he had to go to his room he noticed Natalie's face, which was\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Adam and Natalie are good friends.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Since I have joined Queens University. The time has come to start again. I would love to start an office in my hometown, New York, and start again here.\\n\\nAnd it was for this very reason I had my own mission: I'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Since I have joined Queens University.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Canada is a country..... not just the biggest or most popular in the world, but one of the most unique. Why would we do that? We need to do it without a problem.\\n\\nQ) How would you explain why a small government'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Canada is a country.....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mma865",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
